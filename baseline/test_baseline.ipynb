{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vroom.baseline import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/batum/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Univ/Master 2/S3/Application Innovation/GAN-Character-Networks/baseline/test_baseline.ipynb Cellule 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/batum/Library/Mobile%20Documents/com~apple~CloudDocs/Desktop/Univ/Master%202/S3/Application%20Innovation/GAN-Character-Networks/baseline/test_baseline.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m coocs \u001b[39m=\u001b[39m get_cooccurences_from_text(\u001b[39m\"\u001b[39;49m\u001b[39m../data/kaggle/les_cavernes_d_acier/chapter_3.txt.preprocessed\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/batum/Library/Mobile%20Documents/com~apple~CloudDocs/Desktop/Univ/Master%202/S3/Application%20Innovation/GAN-Character-Networks/baseline/test_baseline.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m coo \u001b[39min\u001b[39;00m coocs:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/batum/Library/Mobile%20Documents/com~apple~CloudDocs/Desktop/Univ/Master%202/S3/Application%20Innovation/GAN-Character-Networks/baseline/test_baseline.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(coo)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Univ/Master 2/S3/Application Innovation/GAN-Character-Networks/vroom/baseline.py:23\u001b[0m, in \u001b[0;36mget_cooccurences_from_text\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cooccurences_from_text\u001b[39m(path: \u001b[39mstr\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m    Get the coocurences of characters from the given text.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m        list: A list of tuples representing the interactions between entities in the text.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     entities, chunks \u001b[39m=\u001b[39m get_entities_from_file(path)\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m get_cooccurences(chunks, entities)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Univ/Master 2/S3/Application Innovation/GAN-Character-Networks/vroom/NER.py:223\u001b[0m, in \u001b[0;36mget_entities_from_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    221\u001b[0m entities \u001b[39m=\u001b[39m []\n\u001b[1;32m    222\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks:\n\u001b[0;32m--> 223\u001b[0m     entities\u001b[39m.\u001b[39mappend(get_entities(chunk))\n\u001b[1;32m    225\u001b[0m \u001b[39mreturn\u001b[39;00m entities, chunks\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Univ/Master 2/S3/Application Innovation/GAN-Character-Networks/vroom/NER.py:44\u001b[0m, in \u001b[0;36mget_entities\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_entities\u001b[39m(text: \u001b[39mstr\u001b[39m):\n\u001b[1;32m     33\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m    Extracts named entities from the given text.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m              'word', 'start', and 'end'.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mJean-Baptiste/camembert-ner\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     45\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     46\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mJean-Baptiste/camembert-ner\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m     nlp \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer, aggregation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msimple\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/ia/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:769\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2043\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2045\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   2046\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2047\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2048\u001b[0m     init_configuration,\n\u001b[1;32m   2049\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   2050\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2051\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2052\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2053\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2054\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   2055\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2056\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2077\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2075\u001b[0m has_tokenizer_file \u001b[39m=\u001b[39m resolved_vocab_files\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtokenizer_file\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m \u001b[39mif\u001b[39;00m (from_slow \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_tokenizer_file) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2077\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m (\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mslow_tokenizer_class)\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   2078\u001b[0m         copy\u001b[39m.\u001b[39;49mdeepcopy(resolved_vocab_files),\n\u001b[1;32m   2079\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2080\u001b[0m         copy\u001b[39m.\u001b[39;49mdeepcopy(init_configuration),\n\u001b[1;32m   2081\u001b[0m         \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   2082\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2083\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2084\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2085\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m   2086\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(copy\u001b[39m.\u001b[39;49mdeepcopy(kwargs)),\n\u001b[1;32m   2087\u001b[0m     )\n\u001b[1;32m   2088\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2089\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   2257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   2258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   2259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/ia/lib/python3.9/site-packages/transformers/models/camembert/tokenization_camembert.py:140\u001b[0m, in \u001b[0;36mCamembertTokenizer.__init__\u001b[0;34m(self, vocab_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, additional_special_tokens, sp_model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model_kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m sp_model_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sp_model_kwargs\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model \u001b[39m=\u001b[39m spm\u001b[39m.\u001b[39mSentencePieceProcessor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model_kwargs)\n\u001b[0;32m--> 140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp_model\u001b[39m.\u001b[39;49mLoad(\u001b[39mstr\u001b[39;49m(vocab_file))\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    143\u001b[0m \u001b[39m# HACK: These tokens were added by the author for an obscure reason as they were already part of the\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m# sentencepiece vocabulary (this is the case for <s> and </s> and <unk>).\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m# In this case it is recommended to properly set the tokens by hand.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ia/lib/python3.9/site-packages/sentencepiece/__init__.py:905\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m model_proto:\n\u001b[1;32m    904\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[0;32m--> 905\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLoadFromFile(model_file)\n",
      "File \u001b[0;32m~/miniforge3/envs/ia/lib/python3.9/site-packages/sentencepiece/__init__.py:310\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadFromFile\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39;49mSentencePieceProcessor_LoadFromFile(\u001b[39mself\u001b[39;49m, arg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coocs = get_cooccurences_from_text(\"../data/kaggle/les_cavernes_d_acier/chapter_3.txt.preprocessed\")\n",
    "for coo in coocs:\n",
    "    print(coo)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliases :  [['Hari Seldon', 'Seldon'], ['Hummin', 'Chetter Hummin'], ['Hélicon'], ['Trantor'], ['Empereur'], ['Demerzel']]\n"
     ]
    }
   ],
   "source": [
    "result = get_cooccurences_with_aliases(\"../data/kaggle/prelude_a_fondation/chapter_3.txt.preprocessed\")\n",
    "# for value in result:\n",
    "#     print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
